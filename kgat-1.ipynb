{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7990392,"sourceType":"datasetVersion","datasetId":4703931},{"sourceId":8008806,"sourceType":"datasetVersion","datasetId":4717134}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-02T14:44:11.087897Z","iopub.execute_input":"2024-04-02T14:44:11.088170Z","iopub.status.idle":"2024-04-02T14:44:12.102348Z","shell.execute_reply.started":"2024-04-02T14:44:11.088146Z","shell.execute_reply":"2024-04-02T14:44:12.101463Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/input/mooccubex/test.txt\n/kaggle/input/mooccubex/kg_final.txt\n/kaggle/input/mooccubex/train.txt\n/kaggle/input/data-mooccubex/test.txt\n/kaggle/input/data-mooccubex/kg_final.txt\n/kaggle/input/data-mooccubex/train.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install gitpython","metadata":{"execution":{"iopub.status.busy":"2024-04-02T14:44:12.103636Z","iopub.execute_input":"2024-04-02T14:44:12.104437Z","iopub.status.idle":"2024-04-02T14:44:24.222514Z","shell.execute_reply.started":"2024-04-02T14:44:12.104382Z","shell.execute_reply":"2024-04-02T14:44:24.221555Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: gitpython in /opt/conda/lib/python3.10/site-packages (3.1.41)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython) (4.0.11)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython) (5.0.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"import git\ngit.Repo.clone_from('https://github.com/VanKhaiii/KGAT_Cube.git', '/kaggle/working/')","metadata":{"execution":{"iopub.status.busy":"2024-04-02T14:44:48.672380Z","iopub.execute_input":"2024-04-02T14:44:48.672987Z","iopub.status.idle":"2024-04-02T14:44:48.752599Z","shell.execute_reply.started":"2024-04-02T14:44:48.672955Z","shell.execute_reply":"2024-04-02T14:44:48.751349Z"},"trusted":true},"execution_count":6,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mGitCommandError\u001b[0m                           Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgit\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mgit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRepo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone_from\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://github.com/VanKhaiii/KGAT_Cube.git\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/working/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/git/repo/base.py:1405\u001b[0m, in \u001b[0;36mRepo.clone_from\u001b[0;34m(cls, url, to_path, progress, env, multi_options, allow_unsafe_protocols, allow_unsafe_options, **kwargs)\u001b[0m\n\u001b[1;32m   1403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1404\u001b[0m     git\u001b[38;5;241m.\u001b[39mupdate_environment(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39menv)\n\u001b[0;32m-> 1405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clone\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mGitCmdObjectDB\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulti_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unsafe_protocols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_unsafe_protocols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unsafe_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_unsafe_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1414\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1415\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/git/repo/base.py:1302\u001b[0m, in \u001b[0;36mRepo._clone\u001b[0;34m(cls, git, url, path, odb_default_type, progress, multi_options, allow_unsafe_protocols, allow_unsafe_options, **kwargs)\u001b[0m\n\u001b[1;32m   1299\u001b[0m     cmdline \u001b[38;5;241m=\u001b[39m remove_password_if_present(cmdline)\n\u001b[1;32m   1301\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCmd(\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms unused stdout: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, cmdline, stdout)\n\u001b[0;32m-> 1302\u001b[0m     \u001b[43mfinalize_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstderr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;66;03m# Our git command could have a different working dir than our actual\u001b[39;00m\n\u001b[1;32m   1305\u001b[0m \u001b[38;5;66;03m# environment, hence we prepend its working dir if required.\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m osp\u001b[38;5;241m.\u001b[39misabs(path):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/git/util.py:483\u001b[0m, in \u001b[0;36mfinalize_process\u001b[0;34m(proc, **kwargs)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wait for the process (clone, fetch, pull or push) and handle its errors accordingly\"\"\"\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;66;03m# TODO: No close proc-streams??\u001b[39;00m\n\u001b[0;32m--> 483\u001b[0m \u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/git/cmd.py:657\u001b[0m, in \u001b[0;36mGit.AutoInterrupt.wait\u001b[0;34m(self, stderr)\u001b[0m\n\u001b[1;32m    655\u001b[0m     errstr \u001b[38;5;241m=\u001b[39m read_all_from_possibly_closed_stream(p_stderr)\n\u001b[1;32m    656\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoInterrupt wait stderr: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (errstr,))\n\u001b[0;32m--> 657\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m GitCommandError(remove_password_if_present(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs), status, errstr)\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m status\n","\u001b[0;31mGitCommandError\u001b[0m: Cmd('git') failed due to: exit code(128)\n  cmdline: git clone -v -- https://github.com/VanKhaiii/KGAT_Cube.git /kaggle/working/\n  stderr: 'fatal: destination path '/kaggle/working' already exists and is not an empty directory.\n'"],"ename":"GitCommandError","evalue":"Cmd('git') failed due to: exit code(128)\n  cmdline: git clone -v -- https://github.com/VanKhaiii/KGAT_Cube.git /kaggle/working/\n  stderr: 'fatal: destination path '/kaggle/working' already exists and is not an empty directory.\n'","output_type":"error"}]},{"cell_type":"code","source":"%cd /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2024-04-02T14:45:09.171358Z","iopub.execute_input":"2024-04-02T14:45:09.171746Z","iopub.status.idle":"2024-04-02T14:45:09.178926Z","shell.execute_reply.started":"2024-04-02T14:45:09.171719Z","shell.execute_reply":"2024-04-02T14:45:09.177896Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**KGAT**","metadata":{}},{"cell_type":"code","source":"!python /kaggle/working/sample/KGAT-pytorch/main_kgat.py --data_dir \"/kaggle/input/\" \\\n                    --data_name data-mooccubex \\\n                    --use_pretrain 0 \\\n                    --n_epoch 10 \\\n                    --cf_batch_size 1024 \\\n                    --kg_batch_size 2048 \\\n                    --test_batch_size 256 \\\n                    --cf_print_every 50 \\\n                    --kg_print_every 50 \\\n                    --evaluate_every 1","metadata":{"execution":{"iopub.status.busy":"2024-04-02T14:45:12.422774Z","iopub.execute_input":"2024-04-02T14:45:12.423121Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"All logs will be saved to trained_model/KGAT/data-mooccubex/embed-dim64_relation-dim64_random-walk_bi-interaction_64-32-16_lr0.0001_pretrain0/log0.log\n2024-04-02 14:45:18,776 - root - INFO - Namespace(seed=2019, data_name='data-mooccubex', data_dir='/kaggle/input/', use_pretrain=0, pretrain_embedding_dir='datasets/pretrain/', pretrain_model_path='trained_model/model.pth', cf_batch_size=1024, kg_batch_size=2048, test_batch_size=256, embed_dim=64, relation_dim=64, laplacian_type='random-walk', aggregation_type='bi-interaction', conv_dim_list='[64, 32, 16]', mess_dropout='[0.1, 0.1, 0.1]', kg_l2loss_lambda=1e-05, cf_l2loss_lambda=1e-05, lr=0.0001, n_epoch=10, stopping_steps=10, cf_print_every=50, kg_print_every=50, evaluate_every=1, Ks='[20, 40, 60, 80, 100]', save_dir='trained_model/KGAT/data-mooccubex/embed-dim64_relation-dim64_random-walk_bi-interaction_64-32-16_lr0.0001_pretrain0/')\n2024-04-02 14:51:39,664 - root - INFO - n_users:           182153\n2024-04-02 14:51:39,664 - root - INFO - n_items:           2947\n2024-04-02 14:51:39,664 - root - INFO - n_entities:        4719\n2024-04-02 14:51:39,664 - root - INFO - n_users_entities:  186872\n2024-04-02 14:51:39,664 - root - INFO - n_relations:       10\n2024-04-02 14:51:39,666 - root - INFO - n_h_list:          8789674\n2024-04-02 14:51:39,666 - root - INFO - n_t_list:          8789674\n2024-04-02 14:51:39,666 - root - INFO - n_r_list:          8789674\n2024-04-02 14:51:39,666 - root - INFO - n_cf_train:        4361721\n2024-04-02 14:51:39,666 - root - INFO - n_cf_test:         566146\n2024-04-02 14:51:39,666 - root - INFO - n_kg_train:        8789674\n/kaggle/working/sample/KGAT-pytorch/data_loader/loader_kgat.py:119: RuntimeWarning: divide by zero encountered in power\n  d_inv = np.power(rowsum, -1.0).flatten()\n/kaggle/working/sample/KGAT-pytorch/data_loader/loader_kgat.py:92: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:605.)\n  return torch.sparse.FloatTensor(i, v, torch.Size(shape))\n/kaggle/working/sample/KGAT-pytorch/model/KGAT.py:111: UserWarning: torch.sparse.SparseTensor(shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(shape, dtype=, device=). (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:634.)\n  self.A_in = nn.Parameter(torch.sparse.FloatTensor(self.n_users + self.n_entities, self.n_users + self.n_entities))\n2024-04-02 14:51:46,433 - root - INFO - KGAT(\n  (entity_user_embed): Embedding(186872, 64)\n  (relation_embed): Embedding(10, 64)\n  (aggregator_layers): ModuleList(\n    (0): Aggregator(\n      (message_dropout): Dropout(p=0.1, inplace=False)\n      (activation): LeakyReLU(negative_slope=0.01)\n      (linear1): Linear(in_features=64, out_features=64, bias=True)\n      (linear2): Linear(in_features=64, out_features=64, bias=True)\n    )\n    (1): Aggregator(\n      (message_dropout): Dropout(p=0.1, inplace=False)\n      (activation): LeakyReLU(negative_slope=0.01)\n      (linear1): Linear(in_features=64, out_features=32, bias=True)\n      (linear2): Linear(in_features=64, out_features=32, bias=True)\n    )\n    (2): Aggregator(\n      (message_dropout): Dropout(p=0.1, inplace=False)\n      (activation): LeakyReLU(negative_slope=0.01)\n      (linear1): Linear(in_features=32, out_features=16, bias=True)\n      (linear2): Linear(in_features=32, out_features=16, bias=True)\n    )\n  )\n)\n2024-04-02 14:51:47,759 - jax._src.path - DEBUG - etils.epath found. Using etils.epath for file I/O.\n2024-04-02 14:52:09,075 - root - INFO - CF Training: Epoch 0001 Iter 0050 / 4260 | Time 0.4s | Iter Loss 0.6970 | Iter Mean Loss 0.6970\n2024-04-02 14:52:27,597 - root - INFO - CF Training: Epoch 0001 Iter 0100 / 4260 | Time 0.4s | Iter Loss 0.6698 | Iter Mean Loss 0.6922\n2024-04-02 14:52:46,107 - root - INFO - CF Training: Epoch 0001 Iter 0150 / 4260 | Time 0.4s | Iter Loss 0.5257 | Iter Mean Loss 0.6644\n2024-04-02 14:53:04,628 - root - INFO - CF Training: Epoch 0001 Iter 0200 / 4260 | Time 0.4s | Iter Loss 0.4521 | Iter Mean Loss 0.6201\n2024-04-02 14:53:23,165 - root - INFO - CF Training: Epoch 0001 Iter 0250 / 4260 | Time 0.4s | Iter Loss 0.4247 | Iter Mean Loss 0.5850\n2024-04-02 14:53:41,697 - root - INFO - CF Training: Epoch 0001 Iter 0300 / 4260 | Time 0.4s | Iter Loss 0.4418 | Iter Mean Loss 0.5580\n2024-04-02 14:54:00,184 - root - INFO - CF Training: Epoch 0001 Iter 0350 / 4260 | Time 0.4s | Iter Loss 0.3917 | Iter Mean Loss 0.5366\n2024-04-02 14:54:18,697 - root - INFO - CF Training: Epoch 0001 Iter 0400 / 4260 | Time 0.4s | Iter Loss 0.3981 | Iter Mean Loss 0.5189\n2024-04-02 14:54:37,192 - root - INFO - CF Training: Epoch 0001 Iter 0450 / 4260 | Time 0.4s | Iter Loss 0.3822 | Iter Mean Loss 0.5046\n2024-04-02 14:54:55,740 - root - INFO - CF Training: Epoch 0001 Iter 0500 / 4260 | Time 0.4s | Iter Loss 0.3737 | Iter Mean Loss 0.4923\n2024-04-02 14:55:14,265 - root - INFO - CF Training: Epoch 0001 Iter 0550 / 4260 | Time 0.4s | Iter Loss 0.3723 | Iter Mean Loss 0.4817\n2024-04-02 14:55:32,762 - root - INFO - CF Training: Epoch 0001 Iter 0600 / 4260 | Time 0.4s | Iter Loss 0.3491 | Iter Mean Loss 0.4726\n2024-04-02 14:55:51,273 - root - INFO - CF Training: Epoch 0001 Iter 0650 / 4260 | Time 0.4s | Iter Loss 0.3440 | Iter Mean Loss 0.4645\n2024-04-02 14:56:09,783 - root - INFO - CF Training: Epoch 0001 Iter 0700 / 4260 | Time 0.4s | Iter Loss 0.3677 | Iter Mean Loss 0.4574\n2024-04-02 14:56:28,305 - root - INFO - CF Training: Epoch 0001 Iter 0750 / 4260 | Time 0.4s | Iter Loss 0.3580 | Iter Mean Loss 0.4511\n2024-04-02 14:56:46,824 - root - INFO - CF Training: Epoch 0001 Iter 0800 / 4260 | Time 0.4s | Iter Loss 0.3414 | Iter Mean Loss 0.4451\n2024-04-02 14:57:05,214 - root - INFO - CF Training: Epoch 0001 Iter 0850 / 4260 | Time 0.4s | Iter Loss 0.3717 | Iter Mean Loss 0.4399\n2024-04-02 14:57:23,661 - root - INFO - CF Training: Epoch 0001 Iter 0900 / 4260 | Time 0.4s | Iter Loss 0.3450 | Iter Mean Loss 0.4349\n2024-04-02 14:57:42,069 - root - INFO - CF Training: Epoch 0001 Iter 0950 / 4260 | Time 0.4s | Iter Loss 0.3632 | Iter Mean Loss 0.4303\n2024-04-02 14:58:00,506 - root - INFO - CF Training: Epoch 0001 Iter 1000 / 4260 | Time 0.4s | Iter Loss 0.3466 | Iter Mean Loss 0.4258\n2024-04-02 14:58:18,940 - root - INFO - CF Training: Epoch 0001 Iter 1050 / 4260 | Time 0.4s | Iter Loss 0.3247 | Iter Mean Loss 0.4218\n2024-04-02 14:58:37,392 - root - INFO - CF Training: Epoch 0001 Iter 1100 / 4260 | Time 0.4s | Iter Loss 0.3179 | Iter Mean Loss 0.4179\n2024-04-02 14:58:55,808 - root - INFO - CF Training: Epoch 0001 Iter 1150 / 4260 | Time 0.4s | Iter Loss 0.3178 | Iter Mean Loss 0.4140\n2024-04-02 14:59:14,243 - root - INFO - CF Training: Epoch 0001 Iter 1200 / 4260 | Time 0.4s | Iter Loss 0.3161 | Iter Mean Loss 0.4105\n2024-04-02 14:59:32,699 - root - INFO - CF Training: Epoch 0001 Iter 1250 / 4260 | Time 0.4s | Iter Loss 0.3292 | Iter Mean Loss 0.4072\n2024-04-02 14:59:51,100 - root - INFO - CF Training: Epoch 0001 Iter 1300 / 4260 | Time 0.4s | Iter Loss 0.3242 | Iter Mean Loss 0.4040\n2024-04-02 15:00:09,564 - root - INFO - CF Training: Epoch 0001 Iter 1350 / 4260 | Time 0.4s | Iter Loss 0.3103 | Iter Mean Loss 0.4009\n2024-04-02 15:00:28,003 - root - INFO - CF Training: Epoch 0001 Iter 1400 / 4260 | Time 0.4s | Iter Loss 0.3286 | Iter Mean Loss 0.3979\n2024-04-02 15:00:46,452 - root - INFO - CF Training: Epoch 0001 Iter 1450 / 4260 | Time 0.4s | Iter Loss 0.3246 | Iter Mean Loss 0.3952\n2024-04-02 15:01:04,895 - root - INFO - CF Training: Epoch 0001 Iter 1500 / 4260 | Time 0.4s | Iter Loss 0.3295 | Iter Mean Loss 0.3926\n2024-04-02 15:01:23,399 - root - INFO - CF Training: Epoch 0001 Iter 1550 / 4260 | Time 0.4s | Iter Loss 0.3184 | Iter Mean Loss 0.3901\n2024-04-02 15:01:41,860 - root - INFO - CF Training: Epoch 0001 Iter 1600 / 4260 | Time 0.4s | Iter Loss 0.3250 | Iter Mean Loss 0.3877\n2024-04-02 15:02:00,265 - root - INFO - CF Training: Epoch 0001 Iter 1650 / 4260 | Time 0.4s | Iter Loss 0.3077 | Iter Mean Loss 0.3853\n2024-04-02 15:02:18,655 - root - INFO - CF Training: Epoch 0001 Iter 1700 / 4260 | Time 0.4s | Iter Loss 0.3014 | Iter Mean Loss 0.3831\n2024-04-02 15:02:37,074 - root - INFO - CF Training: Epoch 0001 Iter 1750 / 4260 | Time 0.4s | Iter Loss 0.3072 | Iter Mean Loss 0.3811\n2024-04-02 15:02:55,477 - root - INFO - CF Training: Epoch 0001 Iter 1800 / 4260 | Time 0.4s | Iter Loss 0.3174 | Iter Mean Loss 0.3790\n2024-04-02 15:03:13,874 - root - INFO - CF Training: Epoch 0001 Iter 1850 / 4260 | Time 0.4s | Iter Loss 0.3150 | Iter Mean Loss 0.3769\n2024-04-02 15:03:32,306 - root - INFO - CF Training: Epoch 0001 Iter 1900 / 4260 | Time 0.4s | Iter Loss 0.2706 | Iter Mean Loss 0.3749\n2024-04-02 15:03:50,691 - root - INFO - CF Training: Epoch 0001 Iter 1950 / 4260 | Time 0.4s | Iter Loss 0.2907 | Iter Mean Loss 0.3729\n2024-04-02 15:04:09,089 - root - INFO - CF Training: Epoch 0001 Iter 2000 / 4260 | Time 0.4s | Iter Loss 0.3009 | Iter Mean Loss 0.3709\n2024-04-02 15:04:27,561 - root - INFO - CF Training: Epoch 0001 Iter 2050 / 4260 | Time 0.4s | Iter Loss 0.3090 | Iter Mean Loss 0.3691\n2024-04-02 15:04:45,939 - root - INFO - CF Training: Epoch 0001 Iter 2100 / 4260 | Time 0.4s | Iter Loss 0.2978 | Iter Mean Loss 0.3673\n2024-04-02 15:05:04,331 - root - INFO - CF Training: Epoch 0001 Iter 2150 / 4260 | Time 0.4s | Iter Loss 0.2885 | Iter Mean Loss 0.3656\n2024-04-02 15:05:22,716 - root - INFO - CF Training: Epoch 0001 Iter 2200 / 4260 | Time 0.4s | Iter Loss 0.2894 | Iter Mean Loss 0.3640\n2024-04-02 15:05:41,136 - root - INFO - CF Training: Epoch 0001 Iter 2250 / 4260 | Time 0.4s | Iter Loss 0.2900 | Iter Mean Loss 0.3624\n2024-04-02 15:05:59,547 - root - INFO - CF Training: Epoch 0001 Iter 2300 / 4260 | Time 0.4s | Iter Loss 0.2941 | Iter Mean Loss 0.3608\n2024-04-02 15:06:17,933 - root - INFO - CF Training: Epoch 0001 Iter 2350 / 4260 | Time 0.4s | Iter Loss 0.2970 | Iter Mean Loss 0.3593\n2024-04-02 15:06:36,365 - root - INFO - CF Training: Epoch 0001 Iter 2400 / 4260 | Time 0.4s | Iter Loss 0.2688 | Iter Mean Loss 0.3578\n2024-04-02 15:06:54,743 - root - INFO - CF Training: Epoch 0001 Iter 2450 / 4260 | Time 0.4s | Iter Loss 0.2764 | Iter Mean Loss 0.3563\n2024-04-02 15:07:13,143 - root - INFO - CF Training: Epoch 0001 Iter 2500 / 4260 | Time 0.4s | Iter Loss 0.2836 | Iter Mean Loss 0.3549\n2024-04-02 15:07:31,556 - root - INFO - CF Training: Epoch 0001 Iter 2550 / 4260 | Time 0.4s | Iter Loss 0.2867 | Iter Mean Loss 0.3536\n2024-04-02 15:07:49,974 - root - INFO - CF Training: Epoch 0001 Iter 2600 / 4260 | Time 0.4s | Iter Loss 0.2789 | Iter Mean Loss 0.3522\n2024-04-02 15:08:08,386 - root - INFO - CF Training: Epoch 0001 Iter 2650 / 4260 | Time 0.4s | Iter Loss 0.3031 | Iter Mean Loss 0.3509\n2024-04-02 15:08:26,832 - root - INFO - CF Training: Epoch 0001 Iter 2700 / 4260 | Time 0.4s | Iter Loss 0.2745 | Iter Mean Loss 0.3497\n2024-04-02 15:08:45,257 - root - INFO - CF Training: Epoch 0001 Iter 2750 / 4260 | Time 0.4s | Iter Loss 0.2668 | Iter Mean Loss 0.3484\n2024-04-02 15:09:03,648 - root - INFO - CF Training: Epoch 0001 Iter 2800 / 4260 | Time 0.4s | Iter Loss 0.3038 | Iter Mean Loss 0.3473\n2024-04-02 15:09:22,028 - root - INFO - CF Training: Epoch 0001 Iter 2850 / 4260 | Time 0.4s | Iter Loss 0.2903 | Iter Mean Loss 0.3462\n2024-04-02 15:09:40,444 - root - INFO - CF Training: Epoch 0001 Iter 2900 / 4260 | Time 0.4s | Iter Loss 0.2736 | Iter Mean Loss 0.3451\n2024-04-02 15:09:58,820 - root - INFO - CF Training: Epoch 0001 Iter 2950 / 4260 | Time 0.4s | Iter Loss 0.2671 | Iter Mean Loss 0.3440\n2024-04-02 15:10:17,211 - root - INFO - CF Training: Epoch 0001 Iter 3000 / 4260 | Time 0.4s | Iter Loss 0.2695 | Iter Mean Loss 0.3429\n2024-04-02 15:10:35,586 - root - INFO - CF Training: Epoch 0001 Iter 3050 / 4260 | Time 0.4s | Iter Loss 0.2747 | Iter Mean Loss 0.3418\n2024-04-02 15:10:53,965 - root - INFO - CF Training: Epoch 0001 Iter 3100 / 4260 | Time 0.4s | Iter Loss 0.2745 | Iter Mean Loss 0.3407\n2024-04-02 15:11:12,402 - root - INFO - CF Training: Epoch 0001 Iter 3150 / 4260 | Time 0.4s | Iter Loss 0.2778 | Iter Mean Loss 0.3397\n2024-04-02 15:11:30,807 - root - INFO - CF Training: Epoch 0001 Iter 3200 / 4260 | Time 0.4s | Iter Loss 0.2786 | Iter Mean Loss 0.3387\n2024-04-02 15:11:49,238 - root - INFO - CF Training: Epoch 0001 Iter 3250 / 4260 | Time 0.4s | Iter Loss 0.2840 | Iter Mean Loss 0.3377\n2024-04-02 15:12:07,589 - root - INFO - CF Training: Epoch 0001 Iter 3300 / 4260 | Time 0.4s | Iter Loss 0.2687 | Iter Mean Loss 0.3367\n2024-04-02 15:12:26,020 - root - INFO - CF Training: Epoch 0001 Iter 3350 / 4260 | Time 0.4s | Iter Loss 0.2711 | Iter Mean Loss 0.3357\n2024-04-02 15:12:44,422 - root - INFO - CF Training: Epoch 0001 Iter 3400 / 4260 | Time 0.4s | Iter Loss 0.2719 | Iter Mean Loss 0.3348\n2024-04-02 15:13:02,801 - root - INFO - CF Training: Epoch 0001 Iter 3450 / 4260 | Time 0.4s | Iter Loss 0.2624 | Iter Mean Loss 0.3339\n2024-04-02 15:13:21,311 - root - INFO - CF Training: Epoch 0001 Iter 3500 / 4260 | Time 0.4s | Iter Loss 0.2700 | Iter Mean Loss 0.3329\n2024-04-02 15:13:39,830 - root - INFO - CF Training: Epoch 0001 Iter 3550 / 4260 | Time 0.4s | Iter Loss 0.2601 | Iter Mean Loss 0.3320\n2024-04-02 15:13:58,293 - root - INFO - CF Training: Epoch 0001 Iter 3600 / 4260 | Time 0.4s | Iter Loss 0.2684 | Iter Mean Loss 0.3311\n2024-04-02 15:14:16,716 - root - INFO - CF Training: Epoch 0001 Iter 3650 / 4260 | Time 0.4s | Iter Loss 0.2522 | Iter Mean Loss 0.3302\n2024-04-02 15:14:35,169 - root - INFO - CF Training: Epoch 0001 Iter 3700 / 4260 | Time 0.4s | Iter Loss 0.2706 | Iter Mean Loss 0.3293\n2024-04-02 15:14:53,611 - root - INFO - CF Training: Epoch 0001 Iter 3750 / 4260 | Time 0.4s | Iter Loss 0.2612 | Iter Mean Loss 0.3284\n2024-04-02 15:15:12,050 - root - INFO - CF Training: Epoch 0001 Iter 3800 / 4260 | Time 0.4s | Iter Loss 0.2560 | Iter Mean Loss 0.3276\n2024-04-02 15:15:30,537 - root - INFO - CF Training: Epoch 0001 Iter 3850 / 4260 | Time 0.4s | Iter Loss 0.2655 | Iter Mean Loss 0.3267\n2024-04-02 15:15:48,944 - root - INFO - CF Training: Epoch 0001 Iter 3900 / 4260 | Time 0.4s | Iter Loss 0.2350 | Iter Mean Loss 0.3259\n2024-04-02 15:16:07,343 - root - INFO - CF Training: Epoch 0001 Iter 3950 / 4260 | Time 0.4s | Iter Loss 0.2610 | Iter Mean Loss 0.3251\n2024-04-02 15:16:25,803 - root - INFO - CF Training: Epoch 0001 Iter 4000 / 4260 | Time 0.4s | Iter Loss 0.2343 | Iter Mean Loss 0.3243\n2024-04-02 15:16:44,233 - root - INFO - CF Training: Epoch 0001 Iter 4050 / 4260 | Time 0.4s | Iter Loss 0.2299 | Iter Mean Loss 0.3235\n2024-04-02 15:17:02,659 - root - INFO - CF Training: Epoch 0001 Iter 4100 / 4260 | Time 0.4s | Iter Loss 0.2417 | Iter Mean Loss 0.3227\n2024-04-02 15:17:21,066 - root - INFO - CF Training: Epoch 0001 Iter 4150 / 4260 | Time 0.4s | Iter Loss 0.2422 | Iter Mean Loss 0.3220\n2024-04-02 15:17:39,484 - root - INFO - CF Training: Epoch 0001 Iter 4200 / 4260 | Time 0.4s | Iter Loss 0.2581 | Iter Mean Loss 0.3212\n2024-04-02 15:17:57,916 - root - INFO - CF Training: Epoch 0001 Iter 4250 / 4260 | Time 0.4s | Iter Loss 0.2498 | Iter Mean Loss 0.3204\n2024-04-02 15:18:01,600 - root - INFO - CF Training: Epoch 0001 Total Iter 4260 | Total Time 1572.1s | Iter Mean Loss 0.3202\n2024-04-02 15:18:06,794 - root - INFO - KG Training: Epoch 0001 Iter 0050 / 4292 | Time 0.1s | Iter Loss 0.6323 | Iter Mean Loss 0.6629\n2024-04-02 15:18:11,759 - root - INFO - KG Training: Epoch 0001 Iter 0100 / 4292 | Time 0.1s | Iter Loss 0.5530 | Iter Mean Loss 0.6283\n2024-04-02 15:18:16,859 - root - INFO - KG Training: Epoch 0001 Iter 0150 / 4292 | Time 0.1s | Iter Loss 0.4633 | Iter Mean Loss 0.5891\n2024-04-02 15:18:21,952 - root - INFO - KG Training: Epoch 0001 Iter 0200 / 4292 | Time 0.1s | Iter Loss 0.3840 | Iter Mean Loss 0.5481\n2024-04-02 15:18:27,009 - root - INFO - KG Training: Epoch 0001 Iter 0250 / 4292 | Time 0.1s | Iter Loss 0.3161 | Iter Mean Loss 0.5082\n2024-04-02 15:18:31,956 - root - INFO - KG Training: Epoch 0001 Iter 0300 / 4292 | Time 0.1s | Iter Loss 0.2652 | Iter Mean Loss 0.4712\n2024-04-02 15:18:36,972 - root - INFO - KG Training: Epoch 0001 Iter 0350 / 4292 | Time 0.1s | Iter Loss 0.2172 | Iter Mean Loss 0.4379\n2024-04-02 15:18:41,973 - root - INFO - KG Training: Epoch 0001 Iter 0400 / 4292 | Time 0.1s | Iter Loss 0.1838 | Iter Mean Loss 0.4082\n2024-04-02 15:18:46,961 - root - INFO - KG Training: Epoch 0001 Iter 0450 / 4292 | Time 0.1s | Iter Loss 0.1549 | Iter Mean Loss 0.3819\n2024-04-02 15:18:51,997 - root - INFO - KG Training: Epoch 0001 Iter 0500 / 4292 | Time 0.1s | Iter Loss 0.1406 | Iter Mean Loss 0.3587\n2024-04-02 15:18:57,111 - root - INFO - KG Training: Epoch 0001 Iter 0550 / 4292 | Time 0.1s | Iter Loss 0.1240 | Iter Mean Loss 0.3380\n2024-04-02 15:19:02,421 - root - INFO - KG Training: Epoch 0001 Iter 0600 / 4292 | Time 0.1s | Iter Loss 0.1058 | Iter Mean Loss 0.3194\n2024-04-02 15:19:07,558 - root - INFO - KG Training: Epoch 0001 Iter 0650 / 4292 | Time 0.1s | Iter Loss 0.0973 | Iter Mean Loss 0.3028\n2024-04-02 15:19:12,704 - root - INFO - KG Training: Epoch 0001 Iter 0700 / 4292 | Time 0.1s | Iter Loss 0.0943 | Iter Mean Loss 0.2878\n2024-04-02 15:19:17,679 - root - INFO - KG Training: Epoch 0001 Iter 0750 / 4292 | Time 0.1s | Iter Loss 0.0826 | Iter Mean Loss 0.2742\n2024-04-02 15:19:22,760 - root - INFO - KG Training: Epoch 0001 Iter 0800 / 4292 | Time 0.1s | Iter Loss 0.0736 | Iter Mean Loss 0.2619\n2024-04-02 15:19:27,820 - root - INFO - KG Training: Epoch 0001 Iter 0850 / 4292 | Time 0.1s | Iter Loss 0.0672 | Iter Mean Loss 0.2506\n2024-04-02 15:19:32,770 - root - INFO - KG Training: Epoch 0001 Iter 0900 / 4292 | Time 0.1s | Iter Loss 0.0592 | Iter Mean Loss 0.2403\n2024-04-02 15:19:37,821 - root - INFO - KG Training: Epoch 0001 Iter 0950 / 4292 | Time 0.1s | Iter Loss 0.0619 | Iter Mean Loss 0.2308\n2024-04-02 15:19:42,830 - root - INFO - KG Training: Epoch 0001 Iter 1000 / 4292 | Time 0.1s | Iter Loss 0.0518 | Iter Mean Loss 0.2221\n2024-04-02 15:19:47,808 - root - INFO - KG Training: Epoch 0001 Iter 1050 / 4292 | Time 0.1s | Iter Loss 0.0493 | Iter Mean Loss 0.2140\n2024-04-02 15:19:52,906 - root - INFO - KG Training: Epoch 0001 Iter 1100 / 4292 | Time 0.1s | Iter Loss 0.0452 | Iter Mean Loss 0.2065\n2024-04-02 15:19:57,914 - root - INFO - KG Training: Epoch 0001 Iter 1150 / 4292 | Time 0.1s | Iter Loss 0.0483 | Iter Mean Loss 0.1995\n2024-04-02 15:20:02,922 - root - INFO - KG Training: Epoch 0001 Iter 1200 / 4292 | Time 0.1s | Iter Loss 0.0391 | Iter Mean Loss 0.1929\n2024-04-02 15:20:07,996 - root - INFO - KG Training: Epoch 0001 Iter 1250 / 4292 | Time 0.1s | Iter Loss 0.0384 | Iter Mean Loss 0.1868\n2024-04-02 15:20:12,974 - root - INFO - KG Training: Epoch 0001 Iter 1300 / 4292 | Time 0.1s | Iter Loss 0.0380 | Iter Mean Loss 0.1811\n2024-04-02 15:20:18,004 - root - INFO - KG Training: Epoch 0001 Iter 1350 / 4292 | Time 0.1s | Iter Loss 0.0367 | Iter Mean Loss 0.1758\n2024-04-02 15:20:23,093 - root - INFO - KG Training: Epoch 0001 Iter 1400 / 4292 | Time 0.1s | Iter Loss 0.0338 | Iter Mean Loss 0.1707\n2024-04-02 15:20:28,196 - root - INFO - KG Training: Epoch 0001 Iter 1450 / 4292 | Time 0.1s | Iter Loss 0.0306 | Iter Mean Loss 0.1660\n2024-04-02 15:20:33,232 - root - INFO - KG Training: Epoch 0001 Iter 1500 / 4292 | Time 0.1s | Iter Loss 0.0271 | Iter Mean Loss 0.1615\n2024-04-02 15:20:38,191 - root - INFO - KG Training: Epoch 0001 Iter 1550 / 4292 | Time 0.1s | Iter Loss 0.0302 | Iter Mean Loss 0.1573\n2024-04-02 15:20:43,293 - root - INFO - KG Training: Epoch 0001 Iter 1600 / 4292 | Time 0.1s | Iter Loss 0.0306 | Iter Mean Loss 0.1533\n2024-04-02 15:20:48,314 - root - INFO - KG Training: Epoch 0001 Iter 1650 / 4292 | Time 0.1s | Iter Loss 0.0301 | Iter Mean Loss 0.1495\n2024-04-02 15:20:53,435 - root - INFO - KG Training: Epoch 0001 Iter 1700 / 4292 | Time 0.1s | Iter Loss 0.0297 | Iter Mean Loss 0.1459\n2024-04-02 15:20:58,548 - root - INFO - KG Training: Epoch 0001 Iter 1750 / 4292 | Time 0.1s | Iter Loss 0.0293 | Iter Mean Loss 0.1425\n2024-04-02 15:21:03,595 - root - INFO - KG Training: Epoch 0001 Iter 1800 / 4292 | Time 0.1s | Iter Loss 0.0228 | Iter Mean Loss 0.1392\n2024-04-02 15:21:08,603 - root - INFO - KG Training: Epoch 0001 Iter 1850 / 4292 | Time 0.1s | Iter Loss 0.0252 | Iter Mean Loss 0.1362\n2024-04-02 15:21:13,689 - root - INFO - KG Training: Epoch 0001 Iter 1900 / 4292 | Time 0.1s | Iter Loss 0.0268 | Iter Mean Loss 0.1332\n2024-04-02 15:21:18,622 - root - INFO - KG Training: Epoch 0001 Iter 1950 / 4292 | Time 0.1s | Iter Loss 0.0261 | Iter Mean Loss 0.1304\n2024-04-02 15:21:23,659 - root - INFO - KG Training: Epoch 0001 Iter 2000 / 4292 | Time 0.1s | Iter Loss 0.0239 | Iter Mean Loss 0.1277\n2024-04-02 15:21:28,683 - root - INFO - KG Training: Epoch 0001 Iter 2050 / 4292 | Time 0.1s | Iter Loss 0.0270 | Iter Mean Loss 0.1251\n2024-04-02 15:21:33,793 - root - INFO - KG Training: Epoch 0001 Iter 2100 / 4292 | Time 0.1s | Iter Loss 0.0213 | Iter Mean Loss 0.1227\n2024-04-02 15:21:38,874 - root - INFO - KG Training: Epoch 0001 Iter 2150 / 4292 | Time 0.1s | Iter Loss 0.0235 | Iter Mean Loss 0.1203\n2024-04-02 15:21:44,064 - root - INFO - KG Training: Epoch 0001 Iter 2200 / 4292 | Time 0.1s | Iter Loss 0.0197 | Iter Mean Loss 0.1180\n2024-04-02 15:21:49,058 - root - INFO - KG Training: Epoch 0001 Iter 2250 / 4292 | Time 0.1s | Iter Loss 0.0182 | Iter Mean Loss 0.1159\n2024-04-02 15:21:54,109 - root - INFO - KG Training: Epoch 0001 Iter 2300 / 4292 | Time 0.1s | Iter Loss 0.0163 | Iter Mean Loss 0.1138\n2024-04-02 15:21:59,259 - root - INFO - KG Training: Epoch 0001 Iter 2350 / 4292 | Time 0.1s | Iter Loss 0.0202 | Iter Mean Loss 0.1118\n2024-04-02 15:22:04,361 - root - INFO - KG Training: Epoch 0001 Iter 2400 / 4292 | Time 0.1s | Iter Loss 0.0206 | Iter Mean Loss 0.1098\n2024-04-02 15:22:09,333 - root - INFO - KG Training: Epoch 0001 Iter 2450 / 4292 | Time 0.1s | Iter Loss 0.0164 | Iter Mean Loss 0.1080\n2024-04-02 15:22:14,376 - root - INFO - KG Training: Epoch 0001 Iter 2500 / 4292 | Time 0.1s | Iter Loss 0.0161 | Iter Mean Loss 0.1062\n2024-04-02 15:22:19,377 - root - INFO - KG Training: Epoch 0001 Iter 2550 / 4292 | Time 0.1s | Iter Loss 0.0201 | Iter Mean Loss 0.1045\n2024-04-02 15:22:24,371 - root - INFO - KG Training: Epoch 0001 Iter 2600 / 4292 | Time 0.1s | Iter Loss 0.0191 | Iter Mean Loss 0.1028\n2024-04-02 15:22:29,415 - root - INFO - KG Training: Epoch 0001 Iter 2650 / 4292 | Time 0.1s | Iter Loss 0.0168 | Iter Mean Loss 0.1012\n2024-04-02 15:22:34,392 - root - INFO - KG Training: Epoch 0001 Iter 2700 / 4292 | Time 0.1s | Iter Loss 0.0126 | Iter Mean Loss 0.0996\n2024-04-02 15:22:39,419 - root - INFO - KG Training: Epoch 0001 Iter 2750 / 4292 | Time 0.1s | Iter Loss 0.0191 | Iter Mean Loss 0.0981\n2024-04-02 15:22:44,579 - root - INFO - KG Training: Epoch 0001 Iter 2800 / 4292 | Time 0.1s | Iter Loss 0.0145 | Iter Mean Loss 0.0967\n2024-04-02 15:22:49,662 - root - INFO - KG Training: Epoch 0001 Iter 2850 / 4292 | Time 0.1s | Iter Loss 0.0132 | Iter Mean Loss 0.0953\n2024-04-02 15:22:54,680 - root - INFO - KG Training: Epoch 0001 Iter 2900 / 4292 | Time 0.1s | Iter Loss 0.0213 | Iter Mean Loss 0.0939\n2024-04-02 15:22:59,786 - root - INFO - KG Training: Epoch 0001 Iter 2950 / 4292 | Time 0.1s | Iter Loss 0.0156 | Iter Mean Loss 0.0926\n2024-04-02 15:23:04,940 - root - INFO - KG Training: Epoch 0001 Iter 3000 / 4292 | Time 0.1s | Iter Loss 0.0173 | Iter Mean Loss 0.0913\n2024-04-02 15:23:09,994 - root - INFO - KG Training: Epoch 0001 Iter 3050 / 4292 | Time 0.1s | Iter Loss 0.0108 | Iter Mean Loss 0.0901\n2024-04-02 15:23:15,067 - root - INFO - KG Training: Epoch 0001 Iter 3100 / 4292 | Time 0.1s | Iter Loss 0.0189 | Iter Mean Loss 0.0889\n2024-04-02 15:23:20,040 - root - INFO - KG Training: Epoch 0001 Iter 3150 / 4292 | Time 0.1s | Iter Loss 0.0124 | Iter Mean Loss 0.0877\n2024-04-02 15:23:25,139 - root - INFO - KG Training: Epoch 0001 Iter 3200 / 4292 | Time 0.1s | Iter Loss 0.0164 | Iter Mean Loss 0.0865\n2024-04-02 15:23:30,284 - root - INFO - KG Training: Epoch 0001 Iter 3250 / 4292 | Time 0.1s | Iter Loss 0.0149 | Iter Mean Loss 0.0854\n2024-04-02 15:23:35,550 - root - INFO - KG Training: Epoch 0001 Iter 3300 / 4292 | Time 0.1s | Iter Loss 0.0142 | Iter Mean Loss 0.0844\n2024-04-02 15:23:40,733 - root - INFO - KG Training: Epoch 0001 Iter 3350 / 4292 | Time 0.1s | Iter Loss 0.0141 | Iter Mean Loss 0.0833\n2024-04-02 15:23:45,826 - root - INFO - KG Training: Epoch 0001 Iter 3400 / 4292 | Time 0.1s | Iter Loss 0.0121 | Iter Mean Loss 0.0823\n2024-04-02 15:23:50,935 - root - INFO - KG Training: Epoch 0001 Iter 3450 / 4292 | Time 0.1s | Iter Loss 0.0136 | Iter Mean Loss 0.0813\n2024-04-02 15:23:56,054 - root - INFO - KG Training: Epoch 0001 Iter 3500 / 4292 | Time 0.1s | Iter Loss 0.0139 | Iter Mean Loss 0.0804\n2024-04-02 15:24:01,148 - root - INFO - KG Training: Epoch 0001 Iter 3550 / 4292 | Time 0.1s | Iter Loss 0.0105 | Iter Mean Loss 0.0794\n2024-04-02 15:24:06,231 - root - INFO - KG Training: Epoch 0001 Iter 3600 / 4292 | Time 0.1s | Iter Loss 0.0120 | Iter Mean Loss 0.0785\n2024-04-02 15:24:11,348 - root - INFO - KG Training: Epoch 0001 Iter 3650 / 4292 | Time 0.1s | Iter Loss 0.0156 | Iter Mean Loss 0.0777\n2024-04-02 15:24:16,367 - root - INFO - KG Training: Epoch 0001 Iter 3700 / 4292 | Time 0.1s | Iter Loss 0.0162 | Iter Mean Loss 0.0768\n2024-04-02 15:24:21,482 - root - INFO - KG Training: Epoch 0001 Iter 3750 / 4292 | Time 0.1s | Iter Loss 0.0144 | Iter Mean Loss 0.0759\n2024-04-02 15:24:26,538 - root - INFO - KG Training: Epoch 0001 Iter 3800 / 4292 | Time 0.1s | Iter Loss 0.0154 | Iter Mean Loss 0.0751\n2024-04-02 15:24:31,647 - root - INFO - KG Training: Epoch 0001 Iter 3850 / 4292 | Time 0.1s | Iter Loss 0.0151 | Iter Mean Loss 0.0743\n2024-04-02 15:24:36,770 - root - INFO - KG Training: Epoch 0001 Iter 3900 / 4292 | Time 0.1s | Iter Loss 0.0133 | Iter Mean Loss 0.0735\n2024-04-02 15:24:41,797 - root - INFO - KG Training: Epoch 0001 Iter 3950 / 4292 | Time 0.1s | Iter Loss 0.0147 | Iter Mean Loss 0.0728\n2024-04-02 15:24:46,855 - root - INFO - KG Training: Epoch 0001 Iter 4000 / 4292 | Time 0.1s | Iter Loss 0.0132 | Iter Mean Loss 0.0720\n2024-04-02 15:24:51,960 - root - INFO - KG Training: Epoch 0001 Iter 4050 / 4292 | Time 0.1s | Iter Loss 0.0143 | Iter Mean Loss 0.0713\n2024-04-02 15:24:57,050 - root - INFO - KG Training: Epoch 0001 Iter 4100 / 4292 | Time 0.1s | Iter Loss 0.0110 | Iter Mean Loss 0.0706\n2024-04-02 15:25:02,175 - root - INFO - KG Training: Epoch 0001 Iter 4150 / 4292 | Time 0.1s | Iter Loss 0.0115 | Iter Mean Loss 0.0699\n2024-04-02 15:25:07,150 - root - INFO - KG Training: Epoch 0001 Iter 4200 / 4292 | Time 0.1s | Iter Loss 0.0123 | Iter Mean Loss 0.0692\n2024-04-02 15:25:12,199 - root - INFO - KG Training: Epoch 0001 Iter 4250 / 4292 | Time 0.1s | Iter Loss 0.0117 | Iter Mean Loss 0.0686\n2024-04-02 15:25:16,412 - root - INFO - KG Training: Epoch 0001 Total Iter 4292 | Total Time 434.8s | Iter Mean Loss 0.0680\ntorch.Size([354])\ntorch.Size([30388])\ntorch.Size([521])\ntorch.Size([1853])\ntorch.Size([354])\ntorch.Size([30388])\ntorch.Size([521])\ntorch.Size([1853])\ntorch.Size([4361721])\ntorch.Size([4361721])\n2024-04-02 15:25:18,660 - root - INFO - Update Attention: Epoch 0001 | Total Time 2.2s\n2024-04-02 15:25:18,662 - root - INFO - CF + KG Training: Epoch 0001 | Total Time 2009.2s\nEvaluating Iteration: 100%|███████████████████| 712/712 [01:57<00:00,  6.08it/s]\n2024-04-02 15:27:15,734 - root - INFO - CF Evaluation: Epoch 0001 | Total Time 117.1s | Precision [0.0122, 0.0091], Recall [0.0962, 0.3147], NDCG [0.0469, 0.1000]\n2024-04-02 15:27:16,110 - root - INFO - Save model on epoch 0001!\n2024-04-02 15:27:31,312 - root - INFO - CF Training: Epoch 0002 Iter 0050 / 4260 | Time 0.3s | Iter Loss 0.3655 | Iter Mean Loss 0.4278\n2024-04-02 15:27:46,482 - root - INFO - CF Training: Epoch 0002 Iter 0100 / 4260 | Time 0.3s | Iter Loss 0.2983 | Iter Mean Loss 0.3724\n2024-04-02 15:28:01,648 - root - INFO - CF Training: Epoch 0002 Iter 0150 / 4260 | Time 0.3s | Iter Loss 0.2827 | Iter Mean Loss 0.3454\n2024-04-02 15:28:16,854 - root - INFO - CF Training: Epoch 0002 Iter 0200 / 4260 | Time 0.3s | Iter Loss 0.2926 | Iter Mean Loss 0.3309\n2024-04-02 15:28:32,083 - root - INFO - CF Training: Epoch 0002 Iter 0250 / 4260 | Time 0.3s | Iter Loss 0.2867 | Iter Mean Loss 0.3208\n2024-04-02 15:28:47,264 - root - INFO - CF Training: Epoch 0002 Iter 0300 / 4260 | Time 0.3s | Iter Loss 0.2824 | Iter Mean Loss 0.3138\n2024-04-02 15:29:02,415 - root - INFO - CF Training: Epoch 0002 Iter 0350 / 4260 | Time 0.3s | Iter Loss 0.2952 | Iter Mean Loss 0.3084\n2024-04-02 15:29:17,550 - root - INFO - CF Training: Epoch 0002 Iter 0400 / 4260 | Time 0.3s | Iter Loss 0.2837 | Iter Mean Loss 0.3041\n2024-04-02 15:29:32,725 - root - INFO - CF Training: Epoch 0002 Iter 0450 / 4260 | Time 0.3s | Iter Loss 0.2767 | Iter Mean Loss 0.3004\n2024-04-02 15:29:47,905 - root - INFO - CF Training: Epoch 0002 Iter 0500 / 4260 | Time 0.3s | Iter Loss 0.2683 | Iter Mean Loss 0.2975\n2024-04-02 15:30:03,066 - root - INFO - CF Training: Epoch 0002 Iter 0550 / 4260 | Time 0.3s | Iter Loss 0.2864 | Iter Mean Loss 0.2946\n2024-04-02 15:30:18,222 - root - INFO - CF Training: Epoch 0002 Iter 0600 / 4260 | Time 0.3s | Iter Loss 0.2588 | Iter Mean Loss 0.2923\n2024-04-02 15:30:33,405 - root - INFO - CF Training: Epoch 0002 Iter 0650 / 4260 | Time 0.3s | Iter Loss 0.2800 | Iter Mean Loss 0.2904\n2024-04-02 15:30:48,579 - root - INFO - CF Training: Epoch 0002 Iter 0700 / 4260 | Time 0.3s | Iter Loss 0.2518 | Iter Mean Loss 0.2885\n2024-04-02 15:31:03,745 - root - INFO - CF Training: Epoch 0002 Iter 0750 / 4260 | Time 0.3s | Iter Loss 0.2689 | Iter Mean Loss 0.2868\n2024-04-02 15:31:18,913 - root - INFO - CF Training: Epoch 0002 Iter 0800 / 4260 | Time 0.3s | Iter Loss 0.2749 | Iter Mean Loss 0.2853\n2024-04-02 15:31:34,072 - root - INFO - CF Training: Epoch 0002 Iter 0850 / 4260 | Time 0.3s | Iter Loss 0.2720 | Iter Mean Loss 0.2841\n2024-04-02 15:31:49,307 - root - INFO - CF Training: Epoch 0002 Iter 0900 / 4260 | Time 0.3s | Iter Loss 0.2672 | Iter Mean Loss 0.2827\n2024-04-02 15:32:04,471 - root - INFO - CF Training: Epoch 0002 Iter 0950 / 4260 | Time 0.3s | Iter Loss 0.2711 | Iter Mean Loss 0.2815\n2024-04-02 15:32:19,646 - root - INFO - CF Training: Epoch 0002 Iter 1000 / 4260 | Time 0.3s | Iter Loss 0.2718 | Iter Mean Loss 0.2803\n2024-04-02 15:32:34,862 - root - INFO - CF Training: Epoch 0002 Iter 1050 / 4260 | Time 0.3s | Iter Loss 0.2492 | Iter Mean Loss 0.2792\n2024-04-02 15:32:50,042 - root - INFO - CF Training: Epoch 0002 Iter 1100 / 4260 | Time 0.3s | Iter Loss 0.2652 | Iter Mean Loss 0.2782\n2024-04-02 15:33:05,215 - root - INFO - CF Training: Epoch 0002 Iter 1150 / 4260 | Time 0.3s | Iter Loss 0.2595 | Iter Mean Loss 0.2772\n2024-04-02 15:33:20,407 - root - INFO - CF Training: Epoch 0002 Iter 1200 / 4260 | Time 0.3s | Iter Loss 0.2515 | Iter Mean Loss 0.2763\n2024-04-02 15:33:35,603 - root - INFO - CF Training: Epoch 0002 Iter 1250 / 4260 | Time 0.3s | Iter Loss 0.2604 | Iter Mean Loss 0.2753\n2024-04-02 15:33:50,803 - root - INFO - CF Training: Epoch 0002 Iter 1300 / 4260 | Time 0.3s | Iter Loss 0.2322 | Iter Mean Loss 0.2744\n2024-04-02 15:34:05,976 - root - INFO - CF Training: Epoch 0002 Iter 1350 / 4260 | Time 0.3s | Iter Loss 0.2468 | Iter Mean Loss 0.2737\n2024-04-02 15:34:21,183 - root - INFO - CF Training: Epoch 0002 Iter 1400 / 4260 | Time 0.3s | Iter Loss 0.2443 | Iter Mean Loss 0.2729\n2024-04-02 15:34:36,389 - root - INFO - CF Training: Epoch 0002 Iter 1450 / 4260 | Time 0.3s | Iter Loss 0.2506 | Iter Mean Loss 0.2721\n2024-04-02 15:34:51,601 - root - INFO - CF Training: Epoch 0002 Iter 1500 / 4260 | Time 0.3s | Iter Loss 0.2524 | Iter Mean Loss 0.2714\n2024-04-02 15:35:06,794 - root - INFO - CF Training: Epoch 0002 Iter 1550 / 4260 | Time 0.3s | Iter Loss 0.2632 | Iter Mean Loss 0.2707\n2024-04-02 15:35:22,002 - root - INFO - CF Training: Epoch 0002 Iter 1600 / 4260 | Time 0.3s | Iter Loss 0.2648 | Iter Mean Loss 0.2699\n2024-04-02 15:35:37,212 - root - INFO - CF Training: Epoch 0002 Iter 1650 / 4260 | Time 0.3s | Iter Loss 0.2697 | Iter Mean Loss 0.2694\n2024-04-02 15:35:52,403 - root - INFO - CF Training: Epoch 0002 Iter 1700 / 4260 | Time 0.3s | Iter Loss 0.2418 | Iter Mean Loss 0.2687\n2024-04-02 15:36:07,551 - root - INFO - CF Training: Epoch 0002 Iter 1750 / 4260 | Time 0.3s | Iter Loss 0.2471 | Iter Mean Loss 0.2682\n2024-04-02 15:36:22,706 - root - INFO - CF Training: Epoch 0002 Iter 1800 / 4260 | Time 0.3s | Iter Loss 0.2476 | Iter Mean Loss 0.2676\n2024-04-02 15:36:37,911 - root - INFO - CF Training: Epoch 0002 Iter 1850 / 4260 | Time 0.3s | Iter Loss 0.2520 | Iter Mean Loss 0.2670\n2024-04-02 15:36:53,122 - root - INFO - CF Training: Epoch 0002 Iter 1900 / 4260 | Time 0.3s | Iter Loss 0.2590 | Iter Mean Loss 0.2665\n2024-04-02 15:37:08,253 - root - INFO - CF Training: Epoch 0002 Iter 1950 / 4260 | Time 0.3s | Iter Loss 0.2476 | Iter Mean Loss 0.2660\n2024-04-02 15:37:23,450 - root - INFO - CF Training: Epoch 0002 Iter 2000 / 4260 | Time 0.3s | Iter Loss 0.2409 | Iter Mean Loss 0.2656\n2024-04-02 15:37:38,584 - root - INFO - CF Training: Epoch 0002 Iter 2050 / 4260 | Time 0.3s | Iter Loss 0.2448 | Iter Mean Loss 0.2651\n2024-04-02 15:37:53,777 - root - INFO - CF Training: Epoch 0002 Iter 2100 / 4260 | Time 0.3s | Iter Loss 0.2452 | Iter Mean Loss 0.2646\n2024-04-02 15:38:08,967 - root - INFO - CF Training: Epoch 0002 Iter 2150 / 4260 | Time 0.3s | Iter Loss 0.2598 | Iter Mean Loss 0.2641\n2024-04-02 15:38:24,203 - root - INFO - CF Training: Epoch 0002 Iter 2200 / 4260 | Time 0.3s | Iter Loss 0.2410 | Iter Mean Loss 0.2636\n2024-04-02 15:38:39,368 - root - INFO - CF Training: Epoch 0002 Iter 2250 / 4260 | Time 0.3s | Iter Loss 0.2572 | Iter Mean Loss 0.2632\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}